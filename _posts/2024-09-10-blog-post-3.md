---
title: 'Self-Reading Note: Canonical Markov Chains and Markov Properties'
date: 2024-09-10
permalink: /posts/2024/09/blog-post-3/
tags:
  - Stochastic Processes
  - Probability Theory
  - Markov Chains
---

# Preliminaries

## Transition Matrix and Markov Kernel
In probability theory, a  **Markov kernel** is a map that in the general theory of  Markov processes plays the role of the transition matrix in the theory of Markov processes with a finite state space ([Wikipedia, Markov kernel](https://en.wikipedia.org/wiki/Markov_kernel)). 

### Markov Kernel / Transition Probability
Let  $\displaystyle(X, \mathcal{A})$  and  $\displaystyle(Y,\mathcal{B})$ be measurable spaces. A  Markov kernel  with source $\displaystyle(X, \mathcal{A})$ and target  $\displaystyle(Y,\mathcal{B})$, is a function $p: X \times \mathcal{B}\mapsto \mathbb{R}$ with the following properties:

1.  For every  $\displaystyle B\in {\mathcal {B}}$, the map  $\displaystyle x\mapsto p (x,B)$  is $\mathcal{A}$-measurable;
2.  For every $\displaystyle x\in X$, the map $\displaystyle B\mapsto p (x,B)$ is a  probability measure on $(Y,\mathcal{B})$.

> **Remark:** In discrete case (MC), we have $\displaystyle \mathbb{P}(X_{n+1}\in A\mid X_{n}=x) = \sum_{x_{i}\in A}p(x,x_{i})$, where in continuous case, it becomes $\displaystyle\int_{A}p(x,dy).$

Moreover, we define the product of two kernels:
> **Definition 1**
> The composition or product of two positive kernels $p$ and $q$ is defined by 
> $$
> pq(x, A) = \displaystyle\int p(x, dy) q(y, A).
> $$

### Extension to Sequence Space and Measures
To define the measure for a sequence, we first extend $\displaystyle(\boldsymbol{\Omega}, \mathcal{F}_{\infty}) = (E^{\mathbb{N}}, \mathcal{E}^{\otimes\mathbb{N}})$

Correspondingly, coordinate mappings $X_n$, are defined by $X_n(\boldsymbol{\omega}) := \omega_n$.

The main goal of this section is to show that with *every transition probability* we may associate a *homogeneous Markov chain*, which will be one of the main objects of our study.

#### Existence of Pointwise Probability Measure
> **Theorem 1** 
> For every $x$ in $E$, there exists a unique probability measure {% raw %} $\mathbb{P_x}$ {% endraw %} on $\displaystyle(\boldsymbol{\Omega}, \mathcal{F_\infty})$ such that for any finite collection $n_0 = 0 < n_1 < n_2 < \cdots < n_k$ of integers and every rectangle 
> $$
> \boldsymbol{A} = \prod_{i=0}^{k}A_{n_i}\prod_{n\neq n_i} E^{(n)},
> $$ 
> {% raw %} 
> $$
> \mathbb{P}_{x}(\boldsymbol{A}) = \mathbb{1}_{A_0}(x) \int_{A_{n_1}}p(x,dx_1)\int_{A_{n_2}}p_{n_2-n_1}(x_1,dx_2)\cdots p_{n_k-n_{k-1}}(x_{k-1}, A_{n_k}).
> $$
> {% endraw %} 
> Furthermore, for every set $A \in \mathcal{F}_{\infty}$, the map $x \mapsto P_x(A)$ is $\mathcal{E}$-measurable.

From the above results, we can prove the existence of a unique probability measure (induced from the provided kernel), under which the coordinate process works as a homogeneous MC.

Then, we can define a measure on sequential $(\boldsymbol{\Omega}, \mathcal{F}_{\infty})$ based on another measure on the basic $(E, \mathcal{E})$.

> **Definition 2** 
> For every probability measure $\mu$ on $(E, \mathcal{E})$, we define a new probability measure $\mathbb{P}_{\mu}$, on $(\boldsymbol{\Omega}, \mathcal{F}_{\infty})$ by setting 
> $$
> \mathbb{P}_\mu(\boldsymbol{A}) = \int \mu(dx) \mathbb{P}_x(A)
> $$

An advantage of building the chain on this canonical probability space is that we can define the shift operators by
$$\theta_k((ω_n)_{n\in \mathbb{Z}_{+}} ) := (ω_{k+n})_{n\in \mathbb{Z}_{+}}$$

# Canonical Markov Chains

## Markov Chain in General Case
We say $X_n$ is a Markov chain (w.r.t. $\mathcal{F}_n$, an example: $\sigma(X_0, \cdots, X_{n})$) with **transition probability** $p$ and **initial distribution** $\mu$ if
$$P_{\mu} (X_{n+1} \in B\mid \mathcal{F}_n) = p(X_n, B).$$

## Markov Properties
> **Theorem 2 (Simple Markov Property)**
>  Let $Y: \boldsymbol{\Omega} → \mathbb{R}$ be bounded (or non-negative) and measurable, an integer $n\geq 0$. 
> $$
> \mathbb{E}_{\mu}[Y\circ\theta_{m}\mid\mathcal{F_m}] = \mathbb{E}_{X_m}[Y],
> $$
>  meaning that the conditional distribution of $\theta_n(\boldsymbol{\omega})$ knowing $(X_0, X_1, \cdots, X_n)$ is $\mathbb{P}_{X_n}$
>  Note that the quantity $\mathbb{E}_X[G]$ is the composition of $X$ with the function $x \mapsto \mathbb{E}_x[G]$.
>  
>  (*Proof.*) We must prove that for any \(B\in\mathcal{F}_{m}\), 
> $$
> \int_{B} Y\circ\theta_{m} d\mathbb{P}_{\mu} = \int_{B} \mathbb{E}_{X_m}[Y] d\mathbb{P}_{\mu}.
> $$ 
> By the usual extension argument, it suffices to prove this relation for the case in which $B$ is a rectangle, and in which $Y = \mathbf{1}_{A}$ where $A$ is the rectangle. The result then follows immediately from convergence theorems.

### Stopping Time
> **Definition 3**
>  A stopping time of the canonical Markov chain $X$ is a random variable defined on $(\boldsymbol{\Omega}, \mathcal{F}_{\infty})$ with the range in $\mathbb{N} \cup \{+\infty\}$ and such that for every integer $n$ the event $\{T = n\}$ is in $\mathcal{F}_n$. The family $\mathcal{F}_{T}$ of events $A$ such that for every $n$, $\{T = n\} \cap A \in \mathcal{F}$, is called the $\sigma$-algebra associated with $T$.

> **Theorem 3 (Strong Markov Property)**
